{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3258e8a",
   "metadata": {},
   "source": [
    "# LangGraph multi-agent design pattern for user-facing apps\n",
    "\n",
    "I tried many of the existing LangGraph design patterns. And while they are great, they lack easy integration into user-facing front-end applications.\n",
    "\n",
    "This design pattern facilitates smooth conversation between the user and various agents on an example of the agents that help the user create tasks in external systems and get information about these tasks.\n",
    "\n",
    "This code can easily be integrated into your user-facing apps like web-chats or Telegram/Whatsapp bots and modified to interact with your task management systems (Asana, Teamwork, Jira, Trello, Monday.com, Basecamp, ClickUp, GoHighLevel (GHL) or any other you use).\n",
    "\n",
    "Adding new functions of the agents is generally as easy as adding new tools in the respective place below.\n",
    "\n",
    "The code is largely based on https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/multi-agent-collaboration.ipynb with additional features for easy integration.\n",
    "\n",
    "Created by Tony AI Champ, August 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05759bd6",
   "metadata": {},
   "source": [
    "## Prepare the stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99bde0a",
   "metadata": {},
   "source": [
    "Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7b6dcc-c985-46e2-8457-7e6b0298b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain langchain_openai langsmith pandas langchain_experimental matplotlib langgraph==0.1.17 langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae26ca6",
   "metadata": {},
   "source": [
    "Import environment variables\n",
    "\n",
    "Make sure you have OPENAI_API_KEY in your .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "743c19df-6da9-4d1e-b2d2-ea40080b9fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3e9b6",
   "metadata": {},
   "source": [
    "## Define assistants and their tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4faa44",
   "metadata": {},
   "source": [
    "Here we define LLMs used for each of the assistants and their system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c57dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "assistants = {\n",
    "    \"HelloAssistant\": {\n",
    "        \"llm\": llm,\n",
    "        \"system_message\": \"\"\"\n",
    "            You are HalloAssistant, one of the assistants from the AI agent team AIPA (AI personal assistants).\n",
    "            Your role is to understand what the user wants to do: reate a new task or get information about an existing one.\n",
    "            Once the user tells they want to create a task, you use switch_assistant tools to switch to TaskSetterAssistant.\n",
    "        \"\"\".replace(\"                \", \"\")\n",
    "    },\n",
    "    \"TaskSetterAssistant\": {\n",
    "        \"llm\": llm,\n",
    "        \"system_message\": \"\"\"\n",
    "            You are a TaskSetterAssistant, one of the assistants from the AI agent team AIPA (AI personal assistants).\n",
    "            Your role is to help the user set a new task.\n",
    "            The process is:\n",
    "            1. get the task description from the user\n",
    "            2. ask clarifying questions\n",
    "            3. provide updated task description\n",
    "            You continue repeating pp.2-3 till the user confirms they are good with the task description.\n",
    "        \"\"\".replace(\"                \", \"\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03811b4",
   "metadata": {},
   "source": [
    "Here we define tools our assistants will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba0f8b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from typing import Annotated, List, TypedDict, Union, Dict\n",
    "from typing_extensions import Literal\n",
    "\n",
    "# This tool is used to switch between the assistants.\n",
    "#  We could use assistants' outputs based on user inputs for that, but in this case this would be difficult to filter out this technical messages\n",
    "#  from the output for the user on the front-end\n",
    "@tool\n",
    "def switch_assistant(\n",
    "    assistant_name: Annotated[Literal[tuple(list(assistants.keys()))], \"The name of the assistant to switch to.\"]\n",
    "):\n",
    "    \"\"\"Use this to switch the current assistant to another one.\"\"\"\n",
    "    return assistant_name\n",
    "\n",
    "# This tool creates a new task\n",
    "@tool\n",
    "def task_create(\n",
    "    task_name: Annotated[str, \"The name of the task.\"],\n",
    "    task_description: Annotated[str, \"The description of the task.\"],\n",
    "):\n",
    "    \"\"\"Use this to create a new task.\"\"\"\n",
    "\n",
    "    # You can change the following code for integration with your task managemnt system like\n",
    "    #   Asana, Teamwork, Jira, Trello, Monday.com, Basecamp, ClickUp, GoHighLevel (GHL) or any other you use.\n",
    "\n",
    "    import os\n",
    "    import uuid\n",
    "    # Create 'tasks' folder if it doesn't exist\n",
    "    tasks_folder = \"tasks\"\n",
    "    os.makedirs(tasks_folder, exist_ok=True)\n",
    "    # Generate a unique filename using UUID\n",
    "    filename = f\"{uuid.uuid4()}.txt\"\n",
    "    file_path = os.path.join(tasks_folder, filename)\n",
    "    # Write task information to the file\n",
    "    result_str = \"\"\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(f\"Task Name: {task_name}\\n\")\n",
    "        file.write(f\"Task Description: {task_description}\\n\")\n",
    "    result_str += f\"\\nTask file created: {file_path}\"\n",
    "\n",
    "    return result_str\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_tasks() -> str:\n",
    "    \"\"\"Use this to get a list of all existing tasks.\"\"\"\n",
    "\n",
    "    import os\n",
    "    tasks_folder = \"tasks\"\n",
    "    tasks = []\n",
    "    if os.path.exists(tasks_folder):\n",
    "        for filename in os.listdir(tasks_folder):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(tasks_folder, filename)\n",
    "                with open(file_path, \"r\") as file:\n",
    "                    content = file.read()\n",
    "                    task_name = content.split(\"Task Name: \")[1].split(\"\\n\")[0]\n",
    "                    tasks.append({\n",
    "                        \"id\": filename.split('.')[0],\n",
    "                        \"name\": task_name,\n",
    "                    })\n",
    "\n",
    "    return str(tasks)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_task_info(task_id: Annotated[str, \"The unique identifier of the task.\"]) -> str:\n",
    "    \"\"\"Use this to get detailed information about a specific task.\"\"\"\n",
    "\n",
    "    import os\n",
    "    tasks_folder = \"tasks\"\n",
    "    file_path = os.path.join(tasks_folder, f\"{task_id}.txt\")\n",
    "    if not os.path.exists(file_path):\n",
    "        return {\"error\": f\"Task with id {task_id} not found.\"}\n",
    "    with open(file_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "        task_name = content.split(\"Task Name: \")[1].split(\"\\n\")[0]\n",
    "        task_description = content.split(\"Task Description: \")[1].strip()\n",
    "    return str({\n",
    "        \"id\": task_id,\n",
    "        \"name\": task_name,\n",
    "        \"description\": task_description\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23eeda7",
   "metadata": {},
   "source": [
    "Adding tools to assistants definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014cfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistants[\"HelloAssistant\"][\"tools\"] = [switch_assistant, list_tasks, get_task_info]\n",
    "assistants[\"TaskSetterAssistant\"][\"tools\"] = [task_create, switch_assistant]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d99878",
   "metadata": {},
   "source": [
    "Defining router that will be used in conditional edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174f7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "RouterOutput = Literal[tuple(list(assistants.keys()))]\n",
    "\n",
    "@staticmethod\n",
    "def router(state) -> RouterOutput:\n",
    "    # if we need to switch back to user\n",
    "    if state[\"next\"] == \"user\":\n",
    "        return \"user\"\n",
    "    # if the last message was sent by an assistant\n",
    "    else:\n",
    "        return state['current_agent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b152f",
   "metadata": {},
   "source": [
    "## MongoDB checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db92bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mongo DB checkpointer\n",
    "\n",
    "# This code is based on https://langchain-ai.github.io/langgraph/how-tos/persistence_mongodb/, but updated to behave async capabilities.\n",
    "\n",
    "import pickle\n",
    "from contextlib import AbstractContextManager\n",
    "from types import TracebackType\n",
    "from typing import Any, Dict, Iterator, Optional, List, AsyncIterator, Tuple\n",
    "from motor.motor_asyncio import AsyncIOMotorClient\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing_extensions import Self\n",
    "\n",
    "from langgraph.checkpoint.base import (\n",
    "    BaseCheckpointSaver,\n",
    "    Checkpoint,\n",
    "    CheckpointMetadata,\n",
    "    CheckpointTuple,\n",
    "    SerializerProtocol,\n",
    ")\n",
    "from langgraph.serde.jsonplus import JsonPlusSerializer\n",
    "\n",
    "class JsonPlusSerializerCompat(JsonPlusSerializer):\n",
    "    \"\"\"A serializer that supports loading pickled checkpoints for backwards compatibility.\"\"\"\n",
    "\n",
    "    def loads(self, data: bytes) -> Any:\n",
    "        if data.startswith(b\"\\x80\") and data.endswith(b\".\"):\n",
    "            return pickle.loads(data)\n",
    "        return super().loads(data)\n",
    "\n",
    "class MongoDBSaver(AbstractContextManager, BaseCheckpointSaver):\n",
    "    \"\"\"A checkpoint saver that stores checkpoints in a MongoDB database.\"\"\"\n",
    "\n",
    "    serde = JsonPlusSerializerCompat()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: AsyncIOMotorClient,\n",
    "        db_name: str,\n",
    "        collection_name: str,\n",
    "        *,\n",
    "        serde: Optional[SerializerProtocol] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(serde=serde)\n",
    "        self.client = client\n",
    "        self.db_name = db_name\n",
    "        self.collection_name = collection_name\n",
    "        self.collection = client[db_name][collection_name]\n",
    "\n",
    "    def __enter__(self) -> Self:\n",
    "        return self\n",
    "\n",
    "    def __exit__(\n",
    "        self,\n",
    "        __exc_type: Optional[type[BaseException]],\n",
    "        __exc_value: Optional[BaseException],\n",
    "        __traceback: Optional[TracebackType],\n",
    "    ) -> Optional[bool]:\n",
    "        return True\n",
    "\n",
    "    async def aget_tuple(self, config: RunnableConfig) -> Optional[CheckpointTuple]:\n",
    "        query = {\"thread_id\": config[\"configurable\"][\"thread_id\"]}\n",
    "        if config[\"configurable\"].get(\"thread_ts\"):\n",
    "            query[\"thread_ts\"] = config[\"configurable\"][\"thread_ts\"]\n",
    "        doc = await self.collection.find_one(query, sort=[(\"thread_ts\", -1)])\n",
    "        if doc:\n",
    "            return CheckpointTuple(\n",
    "                config,\n",
    "                self.serde.loads(doc[\"checkpoint\"]),\n",
    "                self.serde.loads(doc[\"metadata\"]),\n",
    "                (\n",
    "                    {\n",
    "                        \"configurable\": {\n",
    "                            \"thread_id\": doc[\"thread_id\"],\n",
    "                            \"thread_ts\": doc[\"parent_ts\"],\n",
    "                        }\n",
    "                    }\n",
    "                    if doc.get(\"parent_ts\")\n",
    "                    else None\n",
    "                ),\n",
    "            )\n",
    "        return None\n",
    "\n",
    "    async def alist(\n",
    "        self,\n",
    "        config: Optional[RunnableConfig],\n",
    "        *,\n",
    "        filter: Optional[Dict[str, Any]] = None,\n",
    "        before: Optional[RunnableConfig] = None,\n",
    "        limit: Optional[int] = None,\n",
    "    ) -> AsyncIterator[CheckpointTuple]:\n",
    "        query = {}\n",
    "        if config is not None:\n",
    "            query[\"thread_id\"] = config[\"configurable\"][\"thread_id\"]\n",
    "        if filter:\n",
    "            for key, value in filter.items():\n",
    "                query[f\"metadata.{key}\"] = value\n",
    "        if before is not None:\n",
    "            query[\"thread_ts\"] = {\"$lt\": before[\"configurable\"][\"thread_ts\"]}\n",
    "        \n",
    "        cursor = self.collection.find(query).sort(\"thread_ts\", -1)\n",
    "        if limit:\n",
    "            cursor = cursor.limit(limit)\n",
    "        \n",
    "        async for doc in cursor:\n",
    "            yield CheckpointTuple(\n",
    "                {\n",
    "                    \"configurable\": {\n",
    "                        \"thread_id\": doc[\"thread_id\"],\n",
    "                        \"thread_ts\": doc[\"thread_ts\"],\n",
    "                    }\n",
    "                },\n",
    "                self.serde.loads(doc[\"checkpoint\"]),\n",
    "                self.serde.loads(doc[\"metadata\"]),\n",
    "                (\n",
    "                    {\n",
    "                        \"configurable\": {\n",
    "                            \"thread_id\": doc[\"thread_id\"],\n",
    "                            \"thread_ts\": doc[\"parent_ts\"],\n",
    "                        }\n",
    "                    }\n",
    "                    if doc.get(\"parent_ts\")\n",
    "                    else None\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    # Add synchronous version if needed\n",
    "    def list(\n",
    "        self,\n",
    "        config: Optional[RunnableConfig],\n",
    "        *,\n",
    "        filter: Optional[Dict[str, Any]] = None,\n",
    "        before: Optional[RunnableConfig] = None,\n",
    "        limit: Optional[int] = None,\n",
    "    ) -> Iterator[CheckpointTuple]:\n",
    "        import asyncio\n",
    "        return iter(asyncio.run(self.alist_checkpoints(config)))\n",
    "    \n",
    "    async def aput(\n",
    "        self,\n",
    "        config: RunnableConfig,\n",
    "        checkpoint: Checkpoint,\n",
    "        metadata: CheckpointMetadata,\n",
    "    ) -> RunnableConfig:\n",
    "        doc = {\n",
    "            \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "            \"thread_ts\": checkpoint[\"id\"],\n",
    "            \"checkpoint\": self.serde.dumps(checkpoint),\n",
    "            \"metadata\": self.serde.dumps(metadata),\n",
    "        }\n",
    "        if config[\"configurable\"].get(\"thread_ts\"):\n",
    "            doc[\"parent_ts\"] = config[\"configurable\"][\"thread_ts\"]\n",
    "        await self.collection.insert_one(doc)\n",
    "        return {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "                \"thread_ts\": checkpoint[\"id\"],\n",
    "            }\n",
    "        }\n",
    "\n",
    "    async def aget_state(self, config: RunnableConfig) -> Optional[Checkpoint]:\n",
    "        checkpoint_tuple = await self.aget_tuple(config)\n",
    "        if checkpoint_tuple:\n",
    "            return checkpoint_tuple.checkpoint\n",
    "        return None\n",
    "\n",
    "    async def aset_state(self, config: RunnableConfig, state: Checkpoint) -> None:\n",
    "        await self.aput(config, state, {})\n",
    "\n",
    "    async def aput_writes(\n",
    "        self,\n",
    "        config: RunnableConfig,\n",
    "        writes: List[Tuple[str, Any]],\n",
    "        task_id: str,\n",
    "    ) -> None:\n",
    "        thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "        thread_ts = config[\"configurable\"].get(\"thread_ts\")\n",
    "        \n",
    "        doc = {\n",
    "            \"thread_id\": thread_id,\n",
    "            \"task_id\": task_id,\n",
    "            \"writes\": [\n",
    "                (key, self.serde.dumps(value).decode('utf-8'))\n",
    "                for key, value in writes\n",
    "            ],\n",
    "        }\n",
    "        if thread_ts:\n",
    "            doc[\"thread_ts\"] = thread_ts\n",
    "        \n",
    "        await self.collection.insert_one(doc)\n",
    "\n",
    "    async def alist_checkpoints(self, config: Optional[RunnableConfig] = None) -> List[CheckpointTuple]:\n",
    "        return [checkpoint async for checkpoint in self.alist(config)]\n",
    "\n",
    "    async def adelete_checkpoint(self, config: RunnableConfig) -> None:\n",
    "        if config[\"configurable\"].get(\"thread_ts\"):\n",
    "            query = {\n",
    "                \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "                \"thread_ts\": config[\"configurable\"][\"thread_ts\"],\n",
    "            }\n",
    "        else:\n",
    "            query = {\"thread_id\": config[\"configurable\"][\"thread_id\"]}\n",
    "        await self.collection.delete_one(query)\n",
    "\n",
    "    async def aclear(self) -> None:\n",
    "        await self.collection.delete_many({})\n",
    "\n",
    "    # # Synchronous methods (these call their async counterparts)\n",
    "    # def get_tuple(self, config: RunnableConfig) -> Optional[CheckpointTuple]:\n",
    "    #     import asyncio\n",
    "    #     return asyncio.run(self.aget_tuple(config))\n",
    "\n",
    "    def get_tuple(self, config: RunnableConfig) -> Optional[CheckpointTuple]:\n",
    "        import asyncio\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return loop.run_until_complete(self.aget_tuple(config))\n",
    "\n",
    "\n",
    "    # def list(\n",
    "    #     self,\n",
    "    #     config: Optional[RunnableConfig],\n",
    "    #     *,\n",
    "    #     filter: Optional[Dict[str, Any]] = None,\n",
    "    #     before: Optional[RunnableConfig] = None,\n",
    "    #     limit: Optional[int] = None,\n",
    "    # ) -> Iterator[CheckpointTuple]:\n",
    "    #     import asyncio\n",
    "    #     return iter(asyncio.run(self.alist_checkpoints(config)))\n",
    "\n",
    "    def put(\n",
    "        self,\n",
    "        config: RunnableConfig,\n",
    "        checkpoint: Checkpoint,\n",
    "        metadata: CheckpointMetadata,\n",
    "    ) -> RunnableConfig:\n",
    "        import asyncio\n",
    "        return asyncio.run(self.aput(config, checkpoint, metadata))\n",
    "\n",
    "    def get_state(self, config: RunnableConfig) -> Optional[Checkpoint]:\n",
    "        import asyncio\n",
    "        return asyncio.run(self.aget_state(config))\n",
    "\n",
    "    def set_state(self, config: RunnableConfig, state: Checkpoint) -> None:\n",
    "        import asyncio\n",
    "        asyncio.run(self.aset_state(config, state))\n",
    "\n",
    "    def list_checkpoints(self, config: Optional[RunnableConfig] = None) -> List[CheckpointTuple]:\n",
    "        import asyncio\n",
    "        return asyncio.run(self.alist_checkpoints(config))\n",
    "\n",
    "    def delete_checkpoint(self, config: RunnableConfig) -> None:\n",
    "        import asyncio\n",
    "        asyncio.run(self.adelete_checkpoint(config))\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        import asyncio\n",
    "        asyncio.run(self.aclear())\n",
    "\n",
    "\n",
    "# Initialize the MongoDB client\n",
    "client = AsyncIOMotorClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# Create the MongoDBSaver\n",
    "memory = MongoDBSaver(client, \"checkpoints_db\", \"checkpoints_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0810da81",
   "metadata": {},
   "source": [
    "## AIPAAgent class\n",
    "\n",
    "This class creates the agent graph and methods to communicate with the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb8648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIPAAgent class\n",
    "\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from typing import Annotated, List, TypedDict, Union\n",
    "from typing_extensions import Literal\n",
    "\n",
    "import operator\n",
    "\n",
    "import functools\n",
    "\n",
    "import json\n",
    "from pygments import highlight, lexers, formatters\n",
    "\n",
    "\n",
    "\n",
    "class AIPAAgent:\n",
    "    # default model\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "    # this is for storing agent state\n",
    "    checkpointer = None\n",
    "    # this is for retrieving agent state\n",
    "    checkpointer_config = {}\n",
    "\n",
    "    def __init__(self, assistants, llm, router, checkpointer, checkpointer_config):\n",
    "        self.assistants = assistants\n",
    "        self.llm = llm\n",
    "        self.checkpointer = checkpointer\n",
    "        self.checkpointer_config = checkpointer_config\n",
    "        self.router = router\n",
    "\n",
    "        self.tools = [tool for a_name, a_data in self.assistants.items() for tool in a_data[\"tools\"]]\n",
    "\n",
    "        # Creating assistants nodes\n",
    "        for a_name, a_data in self.assistants.items():\n",
    "            agent_creator = self.create_agent(llm=a_data[\"llm\"], tools=a_data[\"tools\"], system_message=a_data[\"system_message\"])\n",
    "            node = functools.partial(self.agent_node, agent_creator=agent_creator, name=a_name)\n",
    "            # globals()[f\"node_{a_name}\"] = node\n",
    "            setattr(self, f\"node_{a_name}\", node)\n",
    "\n",
    "        # Defining agentic workflow\n",
    "\n",
    "        workflow = StateGraph(self.AgentState)\n",
    "\n",
    "        workflow.add_node(\"user\", self.user_node)\n",
    "        workflow.add_node(\"HelloAssistant\", self.node_HelloAssistant)\n",
    "        workflow.add_node(\"TaskSetterAssistant\", self.node_TaskSetterAssistant)\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"user\",\n",
    "            self.router,\n",
    "            {\"user\": \"user\", \"HelloAssistant\": \"HelloAssistant\", \"TaskSetterAssistant\": \"TaskSetterAssistant\"},\n",
    "        )\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"HelloAssistant\",\n",
    "            self.router,\n",
    "            {\"user\": \"user\", \"HelloAssistant\": \"HelloAssistant\", \"TaskSetterAssistant\": \"TaskSetterAssistant\"},\n",
    "        )\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"TaskSetterAssistant\",\n",
    "            self.router,\n",
    "            {\"user\": \"user\", \"TaskSetterAssistant\": \"TaskSetterAssistant\", \"HelloAssistant\": \"HelloAssistant\"},\n",
    "        )\n",
    "\n",
    "        workflow.add_edge(START, \"user\")\n",
    "        self.graph = workflow.compile(checkpointer=checkpointer, interrupt_before=[\"user\"])\n",
    "\n",
    "\n",
    "\n",
    "    # Defining state class for our agents\n",
    "\n",
    "    #  This defines the object that is passed between each node\n",
    "    #  in the graph.\n",
    "    class AgentState(TypedDict):\n",
    "        messages: Annotated[List[Union[BaseMessage, SystemMessage]], operator.add]\n",
    "        # used to switch back to current assistant\n",
    "        current_agent: str\n",
    "        # used when we need to switch the assistant and handout to the user\n",
    "        next: str\n",
    "    \n",
    "\n",
    "    # Defining general function to create an agent\n",
    "\n",
    "    def create_agent(self, llm, tools=[], system_message=\"\"):\n",
    "        def _create_agent():\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\n",
    "                        \"system\",\n",
    "                        \"{system_message}\\n\"\n",
    "                        \" You have access to the following tools: {tool_names}.\\n\",\n",
    "                    ),\n",
    "                    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "                ]\n",
    "            )\n",
    "            prompt = prompt.partial(system_message=system_message)\n",
    "            if tools:\n",
    "                prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "                return prompt | llm.bind_tools(tools)\n",
    "            else:\n",
    "                prompt = prompt.partial(tool_names=\"None\")\n",
    "                return prompt | llm\n",
    "        return _create_agent\n",
    "    \n",
    "\n",
    "    # Defining nodes\n",
    "\n",
    "    # Helper function to create a node for a given agent\n",
    "    def agent_node(self, state, agent_creator, name):\n",
    "        agent = agent_creator()\n",
    "        result = agent.invoke(state)\n",
    "\n",
    "        # executing tools\n",
    "        if result.tool_calls:\n",
    "            tool_messages = []\n",
    "            ret_val = {}\n",
    "            for tool_call in result.tool_calls: \n",
    "                tool_name = tool_call[\"name\"]\n",
    "                tool_args = tool_call[\"args\"]\n",
    "                if tool_name == 'switch_assistant':\n",
    "                    ret_val['current_agent'] = tool_args['assistant_name']\n",
    "                    ret_val['next'] = tool_args['assistant_name']\n",
    "                tool_function = next((tool for tool in self.tools if tool.name == tool_name), None)\n",
    "                tool_output = tool_function.invoke(tool_args)\n",
    "                tool_messages.append(ToolMessage(content=tool_output, name=tool_name, type=\"tool\", tool_call_id=tool_call['id']))\n",
    "            ret_val[\"messages\"] = [result, *tool_messages]\n",
    "            return ret_val\n",
    "\n",
    "        result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name, type=\"ai\")\n",
    "        return {\n",
    "            \"messages\": [result],\n",
    "            \"current_agent\": name,\n",
    "            \"next\": \"user\"\n",
    "        }\n",
    "    \n",
    "    # User node represents input from outside of the graph and is used\n",
    "    # as a breakpoint in the agent work to receive such output\n",
    "    def user_node(self, state):\n",
    "        return\n",
    "\n",
    "\n",
    "    # Helper function for debug purposes. Colorifying JSON.\n",
    "    def json_colorify(self, obj):\n",
    "\n",
    "        def default(o):\n",
    "            if isinstance(o, BaseMessage):\n",
    "                return o.dict()\n",
    "            return str(o)\n",
    "\n",
    "        formatted_json = json.dumps(obj, indent=4, default=default)\n",
    "        colorful_json = highlight(formatted_json,\n",
    "                                lexers.JsonLexer(),\n",
    "                                formatters.TerminalFormatter())\n",
    "        return colorful_json\n",
    "\n",
    "\n",
    "\n",
    "    # Methods to communicate with the graph\n",
    "\n",
    "    # Output method of the graph\n",
    "    async def agent_message(self, config):\n",
    "        \"\"\"Executing agentic workflow and streaming completion tokens.\"\"\"\n",
    "\n",
    "        # init_values is used to set initial state if there is no state in the checkpointer\n",
    "        init_values = {\n",
    "            \"current_agent\": \"HelloAssistant\",\n",
    "            \"next\": \"user\"\n",
    "        # otherwise the init values are None\n",
    "        } if not self.graph.get_state(config).values[\"messages\"] else None\n",
    "\n",
    "        # streaming completion tokens\n",
    "        async for event in self.graph.astream_events(init_values, config, version=\"v1\"):\n",
    "            kind = event[\"event\"]\n",
    "            if kind == \"on_chat_model_stream\":\n",
    "                content = event[\"data\"][\"chunk\"].content\n",
    "                if content:\n",
    "                    yield content\n",
    "\n",
    "\n",
    "    # Input method of the graph\n",
    "    def user_input(self, user_input):\n",
    "        \"\"\"Inserting user input into the graph.\"\"\"\n",
    "\n",
    "        self.graph.update_state(self.checkpointer_config, {\"messages\": [HumanMessage(content=user_input,name=\"user\",type=\"human\")], \"next\": self.graph.get_state(self.checkpointer_config).values['current_agent']}, as_node=\"user\") #\"sender\": \"user\", \n",
    "        return user_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9447e7-9ab6-43eb-8ae6-9b52f8ba8425",
   "metadata": {},
   "source": [
    "## Invoke\n",
    "\n",
    "With the graph created, you can invoke it! Let's have it chart some stats for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176a99b0-b457-45cf-8901-90facaa852da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "User: hi\n",
      "\n",
      "\n",
      "\n",
      "Assistant: Hello! How can I assist you today? Are you looking to create a new task or get information about an existing one?\n",
      "\n",
      "\n",
      "User: list\n",
      "\n",
      "\n",
      "\n",
      "Assistant: Here are your existing tasks:\n",
      "\n",
      "1. **Procure 3D Printing Price in New York** (ID: c0159b37-2225-4a1b-af8a-67dcacf119b6)\n",
      "2. **Procure 3D Printing Price in Jakarta** (ID: e4c68a21-591c-4810-b304-3f6af7cd3f15)\n",
      "\n",
      "Would you like more information about any of these tasks? If so, please provide the task ID.\n",
      "\n",
      "\n",
      "User: information about c0159b37-2225-4a1b-af8a-67dcacf119b6\n",
      "\n",
      "\n",
      "\n",
      "Assistant: Here is the detailed information about the task **\"Procure 3D Printing Price in New York\"**:\n",
      "\n",
      "- **Task ID:** c0159b37-2225-4a1b-af8a-67dcacf119b6\n",
      "- **Description:** \n",
      "  - Procure the price of printing one piece of the 3D model available at [this link](https://www.printables.com/model/152592-honeycomb-storage-wall) (specifically, the file `wall-honeycomb-part.stl`) in New York.\n",
      "  - Consider any available 3D printing services in New York.\n",
      "  - The material and quality of the print can be standard.\n",
      "  - There is no specific deadline for obtaining this information.\n",
      "  - If possible, include price comparisons from multiple providers.\n",
      "\n",
      "Is there anything else you would like to know or do regarding this task?\n",
      "\n",
      "\n",
      "User: no, create a new one\n",
      "\n",
      "\n",
      "\n",
      "Assistant: Great! Let's get started on creating a new task. Could you please provide a description of the new task you have in mind?\n",
      "\n",
      "\n",
      "User: same as c0159b37-2225-4a1b-af8a-67dcacf119b6 but for Brasilia and need to print 10 copies\n",
      "\n",
      "\n",
      "\n",
      "Assistant: Alright, to make sure I have all the details correct for the new task, the description will be:\n",
      "\n",
      "- Procure the price of printing **10 copies** of the 3D model available at [this link](https://www.printables.com/model/152592-honeycomb-storage-wall) (specifically, the file `wall-honeycomb-part.stl`) in **Brasilia**.\n",
      "- Consider any available 3D printing services in Brasilia.\n",
      "- The material and quality of the print can be standard.\n",
      "- There is no specific deadline for obtaining this information.\n",
      "- If possible, include price comparisons from multiple providers.\n",
      "\n",
      "Is there anything else you would like to add or modify?\n",
      "\n",
      "\n",
      "User: all good\n",
      "\n",
      "\n",
      "\n",
      "Assistant: The new task has been successfully created. Here are the details:\n",
      "\n",
      "- **Task Name:** Procure 3D Printing Price in Brasilia\n",
      "- **Description:** \n",
      "  - Procure the price of printing 10 copies of the 3D model available at [this link](https://www.printables.com/model/152592-honeycomb-storage-wall) (specifically, the file `wall-honeycomb-part.stl`) in Brasilia.\n",
      "  - Consider any available 3D printing services in Brasilia.\n",
      "  - The material and quality of the print can be standard.\n",
      "  - There is no specific deadline for obtaining this information.\n",
      "  - If possible, include price comparisons from multiple providers.\n",
      "\n",
      "If there's anything else you need or if you want to create another task, feel free to let me know!\n",
      "\n",
      "\n",
      "User: exit\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main():\n",
    "    # thread_id is basically a unique chat session id\n",
    "    checkpointer_config={\"configurable\": {\"thread_id\": \"306\"}}\n",
    "    agent = AIPAAgent(assistants=assistants, llm=ChatOpenAI(model=\"gpt-4o\"), router=router, checkpointer=memory, checkpointer_config=checkpointer_config)\n",
    "\n",
    "    for i, m in enumerate(agent.graph.get_state(checkpointer_config).values['messages']):\n",
    "        # if m.content and m.type != \"tool\":\n",
    "            # print(f\"{i}: {m.name} ({m.type}): {m.content}\\n\")\n",
    "        print(f\"[{i}] {m}\\n\")\n",
    "\n",
    "    while 1:\n",
    "        if agent.graph.get_state(checkpointer_config).values.get(\"next\") and agent.graph.get_state(checkpointer_config).values.get(\"next\") != \"user\":\n",
    "            print(\"Assistant: \", end=\"\")\n",
    "        try:\n",
    "            async for event in agent.agent_message(checkpointer_config):\n",
    "                print(event, end=\"\")\n",
    "            print(\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"model error: {e}\")\n",
    "\n",
    "        user_input = agent.user_input(input(\"Message: \"))\n",
    "        print(\"User: \", end=\"\")\n",
    "        print(user_input)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        if user_input ==\"exit\":\n",
    "            break\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
